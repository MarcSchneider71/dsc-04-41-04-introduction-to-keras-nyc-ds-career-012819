{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Thus far we have a solid basic conceptual understanding of neural networks and their basic architecture. We've seen nueral networks for classification including a neural network with no hidden layers (logistic regression), one hidden layer, and several hidden layers. From here, we'll begin to use Keras, a package that has prebuilt many of the building blocks of neural networks which we investigated in previous lessons.  \n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "* Explain what a tensor is\n",
    "* Explain and perform tensor slicing\n",
    "* Explain element-wise operations, broadcast operations, and the dot product\n",
    "* Explain the steps to build a neural network in Keras\n",
    "* Explain the concept of batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a step back: tensors and data representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we haven't really discussed the shape of our data in detail.  \n",
    "Let's briefly discuss data representations in neural networks (the building blocks are \"tensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scalars = 0D tensors\n",
    "- Vectors = 1D tensors\n",
    "- Matrices = 2D tensors\n",
    "- 3D tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor is defined by three key attributes:\n",
    "- rank or number of axes\n",
    "- the shape\n",
    "- the data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some important data manipulations in NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unrowing matrices (important for images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eg Santa: `(790, 64, 64, 3)` matrix to a `(64*64*3, 790)` matrix!\n",
    "\n",
    "```{python} \n",
    "img_unrow = img.reshape(790, -1).T  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing the rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vector with `np.shape() (790,)`\n",
    "\n",
    "```{python} \n",
    "np.reshape(vector, (1,790)) \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor slicing\n",
    "\n",
    "We've actually already seen this in previous lectures and labs, although not explicitly. Just like python's native lists, or NumPy arrays, we can slice tensors using the usual syntax:  \n",
    "\n",
    "```tensor[start_idx : end_idx]```.\n",
    "\n",
    "As with lists and NumPy arrays, this will include the start_idx element up to, but not including the end_idx element.\n",
    "\n",
    "Below is a brief example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "Raw Tensor shape: (60000, 28, 28)\n",
      "Tensor Slice [0:100] shape: (100, 28, 28)\n",
      "Tensor Slice [0:100] shape: (100, 28, 28)\n",
      "Tensor Slice [0:100] shape: (100, 28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "digit = train_images[10] #Select an arbitrary case for our example\n",
    "\n",
    "#Checking the shape of our tensor (in this case, the image)\n",
    "print('Raw Tensor shape:', train_images.shape)\n",
    "\n",
    "#Now performing some slices of our image:\n",
    "print('Tensor Slice [0:100] shape:', train_images[:100].shape)\n",
    "\n",
    "#Equivalently\n",
    "print('Tensor Slice [0:100] shape:', train_images[:100, :, :].shape)\n",
    "\n",
    "#Or verbosely:\n",
    "print('Tensor Slice [0:100] shape:', train_images[:100, :28, :28].shape)\n",
    "\n",
    "\n",
    "plt.imshow(digit, cmap=plt.cm.binary) #Display an example image for context\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we sliced our tensor to obtain 100 of the 60,000 images. You can also slice tensors along other axes. For example, the 1st dimension is which image we are referring two, while the 2nd and 3rd axis are the pixels of these images themselves.For example, we could limit the images to the bottom right hand quadrant like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced tensor shape (includes all images but only the lower right hand corner of each: (60000, 14, 14)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADAlJREFUeJzt3V+MnXWdx/H3Z1urUjVQKEY6pEVCussfdzETg7pxN1aTitB6sQkQ2XRXE252VzQmWsKF2btNNEaTNZoGQbISelFxJaQqDWpkg5IOhbCFonSxCyOVDmlWjb0ojd+9mEMyO9I/e57nPDP1934lzcw5PU+/v2n67nPOmXPml6pCUnv+ZKkXIGlpGL/UKOOXGmX8UqOMX2qU8UuNMn6pUcYvNcr4pUatHHLYBRdcUOvXrx/7+BMnTox97MGDB8c+FuDYsWNjH3v++ed3mr1hw4ZOx6sdhw4d4uWXX86Z3HbQ+NevX88jjzwy9vFHjx4d+9gtW7aMfSzAvn37xj72+uuv7zT7rrvu6nS82jE9PX3Gt/Vuv9Qo45ca1Sn+JJuT/CzJwSTb+1qUpMkbO/4kK4CvAB8CLgduSnJ5XwuTNFldzvzvAg5W1XNVdRzYCWztZ1mSJq1L/OuAFxZcnh1dJ+ks0CX+1/pe4h/8WKAktySZSTIzNzfXYZykPnWJfxa4eMHlKeDFxTeqqh1VNV1V02vXru0wTlKfusS/F7gsySVJVgE3Avf3syxJkzb2K/yq6kSSfwS+D6wA7qyqp3pbmaSJ6vTy3qraDezuaS2SBuQr/KRGGb/UKOOXGpUhd+xZvXp1XXHFFWMfPzs7O/axhw8fHvtYgAsvvHDsY1966aVOs6UzNT09zczMzBm9n98zv9Qo45caZfxSo4xfapTxS40yfqlRxi81yvilRhm/1Cjjlxpl/FKjjF9qlPFLjTJ+qVGD7tJ77Ngx9u7dO+TI3nT5seMXXXRRp9kPP/zw2MdeeumlnWbrj5dnfqlRxi81yvilRhm/1KguW3RfnOSHSQ4keSrJrX0uTNJkdXm2/wTw6aral+TNwGNJ9lTV0z2tTdIEjX3mr6rDVbVv9PlvgQO4Rbd01ujl+/xJNgBXA4++xu/dAtzSxxxJ/ekcf5I3Ad8CPllVv1n8+1W1A9gxuu1wmwRIOqVOz/YneR3z4d9TVff1syRJQ+jybH+ArwMHquqL/S1J0hC6nPnfC/wt8P4kT4x+XdvTuiRN2NiP+avqP4Az2hNM0vLjK/ykRhm/1KhB389/NuuylfmmTZs6zfY9+ZoEz/xSo4xfapTxS40yfqlRxi81yvilRhm/1Cjjlxpl/FKjjF9qlPFLjTJ+qVHGLzXK+KVGnVVv6d25c+fYx1511VU9ruT/Z2pqaslmSyfjmV9qlPFLjTJ+qVHGLzWqc/xJViR5PMkDfSxI0jD6OPPfyvwOvZLOIl336psCPgzc0c9yJA2l65n/S8BngN+f7AZJbkkyk2Sm4yxJPeqyUed1wJGqeuxUt6uqHVU1XVXT486S1L+uG3VuSXII2Mn8hp3f7GVVkiZu7Pir6raqmqqqDcCNwA+q6ubeViZpovw+v9SoXt7YU1U/An7Ux58laRie+aVGGb/UqEHfz79q1apO722/4YYbelyN1DbP/FKjjF9qlPFLjTJ+qVHGLzXK+KVGGb/UKOOXGmX8UqOMX2qU8UuNMn6pUcYvNcr4pUYN+pbejRs3snv37iFHSjoJz/xSo4xfapTxS40yfqlRXTfqPDfJriTPJDmQ5N19LUzSZHV9tv/LwPeq6m+SrALO6WFNkgYwdvxJ3gK8D/g7gKo6DhzvZ1mSJq3L3f63A3PAXUkeT3JHktWLb7Rwi+6jR492GCepT13iXwm8E/hqVV0N/A7YvvhGC7foXrNmTYdxkvrUJf5ZYLaqHh1d3sX8fwaSzgJdtuj+FfBCko2jqzYBT/eyKkkT1/XZ/n8C7hk90/8c8PfdlyRpCJ3ir6ongOme1iJpQL7CT2qU8UuNOqu26JbUH8/8UqOMX2qU8UuNMn6pUcYvNcr4pUYZv9Qo45caZfxSo4xfapTxS40yfqlRxi81yvilRhm/1Cjjlxpl/FKjjF9qlPFLjeq6RfenkjyVZH+Se5O8oa+FSZqsseNPsg74BDBdVVcCK4Ab+1qYpMnqerd/JfDGJCuBc4AXuy9J0hC67NX3S+ALwPPAYeDXVfXg4tst3KJ7bm5u/JVK6lWXu/3nAVuBS4CLgNVJbl58u4VbdK9du3b8lUrqVZe7/R8AflFVc1X1CnAf8J5+liVp0rrE/zxwTZJzkoT5LboP9LMsSZPW5TH/o8AuYB/wn6M/a0dP65I0YV236P4c8Lme1iJpQL7CT2qU8UuNMn6pUcYvNcr4pUYZv9Qo45caZfxSo4xfapTxS40yfqlRxi81yvilRhm/1Cjjlxpl/FKjjF9qlPFLjTJ+qVHGLzXK+KVGGb/UqNPGn+TOJEeS7F9w3Zoke5I8O/p43mSXKalvZ3Lm/wawedF124GHquoy4KHRZUlnkdPGX1U/Bo4uunorcPfo87uBj/S8LkkTNu5j/rdW1WGA0ccLT3ZDt+iWlqeJP+HnFt3S8jRu/C8leRvA6OOR/pYkaQjjxn8/sG30+TbgO/0sR9JQzuRbffcCPwE2JplN8nHgX4APJnkW+ODosqSzyGm36K6qm07yW5t6XoukAfkKP6lRxi81yvilRhm/1Cjjlxpl/FKjjF9qlPFLjTJ+qVHGLzXK+KVGGb/UKOOXGmX8UqOMX2qU8UuNMn6pUcYvNcr4pUYZv9Qo45caZfxSo8bdovvzSZ5J8mSSbyc5d7LLlNS3cbfo3gNcWVXvAH4O3NbzuiRN2FhbdFfVg1V1YnTxp8DUBNYmaYL6eMz/MeC7Pfw5kgbUKf4ktwMngHtOcZtbkswkmZmbm+syTlKPxo4/yTbgOuCjVVUnu11V7aiq6aqaXrt27bjjJPXstBt1vpYkm4HPAn9VVcf6XZKkIYy7Rfe/Am8G9iR5IsnXJrxOST0bd4vur09gLZIG5Cv8pEYZv9Qo45caZfxSo4xfapTxS40yfqlRxi81yvilRhm/1Cjjlxpl/FKjjF9qlPFLjTJ+qVHGLzXK+KVGGb/UqJziB+/2PyyZA/77FDe5AHh5oOU429l/jLPXV9UZ/ZjsQeM/nSQzVTXtbGc7e/K82y81yvilRi23+Hc429nOHsayeswvaTjL7cwvaSDLIv4km5P8LMnBJNsHnHtxkh8mOZDkqSS3DjV7wRpWJHk8yQMDzz03ya4kz4y+/ncPOPtTo7/v/UnuTfKGCc+7M8mRJPsXXLcmyZ4kz44+njfg7M+P/t6fTPLtJOdOYvbpLHn8SVYAXwE+BFwO3JTk8oHGnwA+XVV/BlwD/MOAs191K3Bg4JkAXwa+V1V/Cvz5UGtIsg74BDBdVVcCK4AbJzz2G8DmRddtBx6qqsuAh0aXh5q9B7iyqt4B/By4bUKzT2nJ4wfeBRysqueq6jiwE9g6xOCqOlxV+0af/5b5ANYNMRsgyRTwYeCOoWaO5r4FeB+jPRer6nhV/c+AS1gJvDHJSuAc4MVJDquqHwNHF129Fbh79PndwEeGml1VD1bVidHFnwJTk5h9Ossh/nXACwsuzzJggK9KsgG4Gnh0wLFfAj4D/H7AmQBvB+aAu0YPOe5IsnqIwVX1S+ALwPPAYeDXVfXgELMXeWtVHR6t6TBw4RKsAeBjwHeXYvByiD+vcd2g34JI8ibgW8Anq+o3A828DjhSVY8NMW+RlcA7ga9W1dXA75jc3d7/Y/TYeitwCXARsDrJzUPMXm6S3M78Q897lmL+coh/Frh4weUpJnw3cKEkr2M+/Huq6r6h5gLvBbYkOcT8Q533J/nmQLNngdmqevVezi7m/zMYwgeAX1TVXFW9AtwHvGeg2Qu9lORtAKOPR4YcnmQbcB3w0Vqi77cvh/j3ApcluSTJKuaf/Ll/iMFJwvzj3gNV9cUhZr6qqm6rqqmq2sD81/yDqhrkDFhVvwJeSLJxdNUm4OkhZjN/d/+aJOeM/v43sTRPeN4PbBt9vg34zlCDk2wGPgtsqapjQ839A1W15L+Aa5l/1vO/gNsHnPuXzD/EeBJ4YvTr2iX4+v8aeGDgmX8BzIy+9n8Hzhtw9j8DzwD7gX8DXj/hefcy//zCK8zf6/k4cD7zz/I/O/q4ZsDZB5l/nuvVf3NfG/rfXFX5Cj+pVcvhbr+kJWD8UqOMX2qU8UuNMn6pUcYvNcr4pUYZv9So/wVVMnP5gZCUZAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lower_right_quadrant = train_images[:,14:,14:]\n",
    "print('Sliced tensor shape (includes all images but only the lower right hand corner of each:',\n",
    "      lower_right_quadrant.shape)\n",
    "plt.imshow(lower_right_quadrant[9], cmap=plt.cm.binary) #Display the 10th image from our sliced tensor.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Operations\n",
    "    * Element-wise\n",
    "    * Broadcasting\n",
    "    * Tensor Dot\n",
    "    \n",
    "These are the three main operations that you will see in future implementations. Element-wise addition (or other operations) simply updates each element with the corresponding element from another tensor. For example, here is the result of an element-wise addition of two NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  8, 10, 12])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([1,2,3,4]) + np.array([5,6,7,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting operations can be used when tensors are of different dimensions. For example, we could add the vector [1,2,3] to a 3 by 4 matrix like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]] \n",
      "\n",
      "B: [1 2 3] \n",
      "\n",
      "Updated A:\n",
      " [[ 1  3  5]\n",
      " [ 4  6  8]\n",
      " [ 7  9 11]\n",
      " [10 12 14]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array(range(12)).reshape(4,3)\n",
    "print('A:\\n', A, '\\n')\n",
    "\n",
    "B = np.array([1,2,3])#.reshape(1, -1)\n",
    "print('B:', B, '\\n')\n",
    "\n",
    "A += B #Update with broadcasting\n",
    "print('Updated A:\\n', A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticed how B was broadcasted across A; a copy of B was added to each row in A.\n",
    "Finally, as with our previous work with linear algebra, we will also commonly take the dot product of tensors.\n",
    "Recall that this is the sum of the element-wise products. Let's start with a very simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "#Recall that B is the vector [1,2,3]\n",
    "#Taking the dot product of B and itself is equivalent to\n",
    "#1*1 + 2*2 + 3*3 = 1 + 4 + 9 = 14\n",
    "print(np.dot(B,B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining a slightly more complex example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]] \n",
      "\n",
      "B: [1 2 3] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 8, 26, 44, 62])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array(range(12)).reshape(4,3)\n",
    "print('A:\\n', A, '\\n')\n",
    "\n",
    "B = np.array([1,2,3])#.reshape(1, -1)\n",
    "print('B:', B, '\\n')\n",
    "\n",
    "np.dot(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the first element is the sum of the first row of A multiplied by B elementwise:  \n",
    "0*1 + 1*2 + 2*3 = 0 + 2 + 6 = 8  \n",
    "\n",
    "Followed by the sum of the second row of A multiplied by B elementwise:  \n",
    "3*1 + 4*2 + 5*3 = 3 + 8 + 15 = 26\n",
    "\n",
    "and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the packages\n",
    "\n",
    "As usual, we need to import some packages..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding on the network architecture\n",
    "\n",
    "Afterwards, we define the type of network. We will discuss other network types later, but to date, we have examined sequential models; one layer builds upon the previous one and continues to chain until the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{python}\n",
    "model = models.Sequential()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding layers\n",
    "\n",
    "Once we have initialized a network object as shown above, we can then add layers to the network which includes the number of layers we wish to add, as well as which activiation function we hope to use. For example, when coding from scratch, we previously used the sigmoid and ReLu activation functions.   \n",
    "\n",
    "The `Dense` method indicates that this layer will be fully connected. There are other layer architectures that we will discuss further in upcoming labs and lessons.\n",
    "\n",
    "Finally, the `input_shape` parameter is often optional. That is, in successive layers, Keras implies the required shape of the layer to be added based on the shape of the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{python}\n",
    "model.add(layers.Dense(units, activation, input_shape))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model\n",
    "\n",
    "Once we have defined the network architecture and added layers to that network, we then compile the model before then training that model on our data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{python}\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this is also where we define our loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{python}\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why batches? if you push all your samples through at once, you have to wait until everything is processed and can only start backpropagating then. Therefore, batches are used, so that after each batch has done a forward propagation step, backward propagation can happen again. In essence, it's \"mini-batch\" gradient descent.\n",
    "\n",
    "\n",
    "Here's some further notes regarding these terms from the Keras documentation FAQ:\n",
    "\n",
    "\n",
    "* **Sample**: one element of a dataset.  \n",
    "    * *Example*: one image is a sample in a convolutional network  \n",
    "    * *Example*: one audio file is a sample for a speech recognition model  \n",
    "    \n",
    "* **Batch**: a set of N samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model.  \n",
    "* A batch generally approximates the distribution of the input data better than a single input. The larger the batch, the better the approximation; however, it is also true that the batch will take longer to process and will still result in only one update. For inference (evaluate/predict), it is recommended to pick a batch size that is as large as you can afford without going out of memory (since larger batches will usually result in faster evaluation/prediction).\n",
    "* **Epoch**: an arbitrary cutoff, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation.\n",
    "* When using validation_data or validation_split with the fit method of Keras models, evaluation will be run at the end of every epoch.\n",
    "* Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting \n",
    "\n",
    "When we fit the model as shown above, we not only update the model object itself, we are also returned a history associated with the model. (Hence our variable name.) With this, we can retrieve further information regarding how the model training progressed from epoch to epoch. To do this, you can access the history attribute of the returned object. Given our variable naming above, we would thus have:\n",
    "\n",
    "```history.history```\n",
    "\n",
    "This will return a dictionary of the metrics we indicated when compiling the model. By default, the loss criteria will always be included as well. So in our example, this dictionary will have 2 keys, one for the loss, and one for the accuracy. If you wish to plot learning curves for the loss or accuracy versus the epochs, you can then simply retrieve these lists. For example:\n",
    "\n",
    "```history.history['loss']```\n",
    "\n",
    "would return a list of the loss at each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "As with sci-kit learn and other prebuilt packages, making predictions from a trained model is relatively straightforward. To do this, you can simply use the `predict` method built into the model object. For example:  \n",
    "```{python}\n",
    "y_hat = model.predict(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "Now that the model has been trained, our predictions are applying that model to the data. Similarly, we can use the `evaluate` method in order to compute the loss and other specified metrics for our trained model.\n",
    "\n",
    "For example,   \n",
    "\n",
    "```model.evaluate(X_train, X_train_labels)``` will return the final loss associated with the model for the training data as well as any other metrics that were specified during compilation.\n",
    "\n",
    "Similarly, \n",
    "\n",
    "```model.evaluate(X_test, X_test_labels)``` will return the final loss associated with the model for the test data as well as any other specified metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "    \n",
    "* https://keras.io/getting-started/\n",
    "* https://keras.io/getting-started/sequential-model-guide/#compilation\n",
    "* https://www.coursera.org/learn/deep-neural-network/lecture/BhJlm/rmsprop\n",
    "* https://www.coursera.org/learn/deep-neural-network/lecture/qcogH/mini-batch-gradient-descent\n",
    "* A full book on Keras by the author of Keras himself:  \n",
    "    https://www.manning.com/books/deep-learning-with-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lesson, we began to introduce some of the basic components of building a neural network using Keras. In the upcoming lab you will begin to put these concepts into practice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
